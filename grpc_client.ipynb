{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-serving-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import io\n",
    "import argparse\n",
    "import sys\n",
    "from openvino.inference_engine import IECore\n",
    "import IPython.display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import datetime\n",
    "import grpc\n",
    "from tensorflow import make_tensor_proto, make_ndarray\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc, get_model_metadata_pb2\n",
    "\n",
    "from tensorflow.keras.applications.resnet import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        #Read in Labels\n",
    "        arg_labels=\"train_data/labels.txt\"\n",
    "        label_file = open(arg_labels, \"r\")\n",
    "        self.labels = label_file.read().split('\\n')\n",
    "\n",
    "    def predict(self, imageFile):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class RemodeOpenVINOModel(Model):        \n",
    "    def __init__(self, grpc_address='localhost', grpc_port=9000, model_name='dogcat', model_version=None):\n",
    "        super(RemodeOpenVINOModel, self).__init__()\n",
    "        \n",
    "        #Settings for accessing model server\n",
    "        self.grpc_address = grpc_address\n",
    "        self.grpc_port = grpc_port\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        channel = grpc.insecure_channel(\"{}:{}\".format(self.grpc_address, self.grpc_port))\n",
    "        self.stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "        \n",
    "        # Get input shape info from Model Server\n",
    "        self.input_name, input_shape, self.output_name, output_shape = self.__get_input_name_and_shape__()\n",
    "        self.input_height = input_shape[2]\n",
    "        self.input_width = input_shape[3]\n",
    "        \n",
    "    def __get_input_name_and_shape__(self):\n",
    "        metadata_field = \"signature_def\"\n",
    "        request = get_model_metadata_pb2.GetModelMetadataRequest()\n",
    "        request.model_spec.name = self.model_name\n",
    "        if self.model_version is not None:\n",
    "            request.model_spec.version.value = self.model_version\n",
    "        request.metadata_field.append(metadata_field)\n",
    "\n",
    "        result = self.stub.GetModelMetadata(request, 10.0) # result includes a dictionary with all model outputs\n",
    "        input_metadata, output_metadata = self.__get_input_and_output_meta_data__(result)\n",
    "        input_blob = next(iter(input_metadata.keys()))\n",
    "        output_blob = next(iter(output_metadata.keys()))\n",
    "        return input_blob, input_metadata[input_blob]['shape'], output_blob, output_metadata[output_blob]['shape']\n",
    "    \n",
    "    def __get_input_and_output_meta_data__(self, response):\n",
    "        signature_def = response.metadata['signature_def']\n",
    "        signature_map = get_model_metadata_pb2.SignatureDefMap()\n",
    "        signature_map.ParseFromString(signature_def.value)\n",
    "        serving_default = signature_map.ListFields()[0][1]['serving_default']\n",
    "        serving_inputs = serving_default.inputs\n",
    "        input_blobs_keys = {key: {} for key in serving_inputs.keys()}\n",
    "        tensor_shape = {key: serving_inputs[key].tensor_shape\n",
    "                        for key in serving_inputs.keys()}\n",
    "        for input_blob in input_blobs_keys:\n",
    "            inputs_shape = [d.size for d in tensor_shape[input_blob].dim]\n",
    "            tensor_dtype = serving_inputs[input_blob].dtype\n",
    "            input_blobs_keys[input_blob].update({'shape': inputs_shape})\n",
    "            input_blobs_keys[input_blob].update({'dtype': tensor_dtype})\n",
    "        \n",
    "        serving_outputs = serving_default.outputs\n",
    "        output_blobs_keys = {key: {} for key in serving_outputs.keys()}\n",
    "        tensor_shape = {key: serving_outputs[key].tensor_shape\n",
    "                        for key in serving_outputs.keys()}\n",
    "        for output_blob in output_blobs_keys:\n",
    "            outputs_shape = [d.size for d in tensor_shape[output_blob].dim]\n",
    "            tensor_dtype = serving_outputs[output_blob].dtype\n",
    "            output_blobs_keys[output_blob].update({'shape': outputs_shape})\n",
    "            output_blobs_keys[output_blob].update({'dtype': tensor_dtype})\n",
    "\n",
    "        return input_blobs_keys, output_blobs_keys\n",
    "    \n",
    "    def predict(self, imageFile):\n",
    "        start1 = time.time() #ここ追加\n",
    "        \n",
    "        image = cv2.imread(imageFile)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (self.input_width, self.input_height))\n",
    "        frame = image\n",
    "        image = preprocess_input(image)\n",
    "        image = image.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "        image = image.reshape((1, 3, self.input_height, self.input_width))\n",
    "        images = image\n",
    "        input_image = images.astype(np.float32)\n",
    "\n",
    "        start2 = time.time() #ここ追加\n",
    "        #predictions = self.exec_net.infer(inputs={self.input_blob: images})\n",
    "        # Model ServerにgRPCでアクセスしてモデルをコール\n",
    "        request = predict_pb2.PredictRequest()\n",
    "        request.model_spec.name = self.model_name\n",
    "        request.inputs[self.input_name].CopyFrom(make_tensor_proto(input_image, shape=(input_image.shape)))\n",
    "        result = self.stub.Predict(request, 10.0) # result includes a dictionary with all model outputs\n",
    "        predictions = make_ndarray(result.outputs[self.output_name])\n",
    "        \n",
    "        infer_time = time.time() - start2\n",
    "\n",
    "        # Print the highest probability label\n",
    "        #predictions = predictions[self.out_blob]\n",
    "        highest_probability_index = predictions[0].argsort()[-1:][::-1][0]\n",
    "\n",
    "        total_time = time.time() - start1\n",
    "\n",
    "        #return total_time, infer_time, self.labels[highest_probability_index], frame  #ここ追加\n",
    "        return total_time, infer_time, \"\", frame  #ここ追加\n",
    "\n",
    "\n",
    "def run_inference(grpc_address='161.202.224.155', grpc_port='9000', model_name='dogcat', total=500):\n",
    "    model = RemodeOpenVINOModel(grpc_address, grpc_port, model_name)\n",
    "\n",
    "    total_infer_spent_time = 0\n",
    "    total_spent_time = 0\n",
    "    list_df = pd.DataFrame( columns=['正解ラベル','予測ラベル','全処理時間(msec)','推論時間(msec)'] )\n",
    "\n",
    "    match = 0\n",
    "    #file_list = glob.glob(os.path.join(dataset_dir, \"*\"))\n",
    "    file_list = glob.glob(\"train_data/test/*/*\")\n",
    "    for i in range(total):\n",
    "        img_path = random.choice(file_list)\n",
    "        img_cat = os.path.split(os.path.dirname(img_path))[1]\n",
    "        total_time, infer_time, pred_label, frame = model.predict(img_path)\n",
    "\n",
    "        if i > 1:\n",
    "            total_infer_spent_time += infer_time\n",
    "            total_spent_time += total_time\n",
    "\n",
    "        #print(img_path, str(int(total_time*1000.0)) + 'msec', str(int(infer_time*1000.0)) + 'msec', pred_label) #ここ追加\n",
    "        clear_output(wait=True)\n",
    "        frame = cv2.imread(img_path)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        if frame.shape[:-1] != (224, 224):\n",
    "            frame = cv2.resize(frame, (224, 224))\n",
    "        cv2.putText(frame,'No ' + str(i+1) + ':' + str(int(total_time*1000)) + ',' + str(int(infer_time*1000)), (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,2550), 4)\n",
    "        cv2.putText(frame,'No ' + str(i+1) + ':' + str(int(total_time*1000)) + ',' + str(int(infer_time*1000)), (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "        cv2.putText(frame,str(img_cat), (10,80), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,2550), 4)\n",
    "        cv2.putText(frame,str(img_cat), (10,80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "        cv2.putText(frame,str(pred_label), (10,130), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,2550), 4)\n",
    "        cv2.putText(frame,str(pred_label), (10,130), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 2)\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(frame).save(f, 'jpeg')\n",
    "        IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "\n",
    "        tmp_se = pd.Series( [img_cat, pred_label, str(int(total_time * 1000)), str(int(infer_time * 1000)) ], index=list_df.columns )\n",
    "        list_df = list_df.append( tmp_se, ignore_index=True ) \n",
    "\n",
    "    print()\n",
    "    print('全' + str(total) + '枚 完了！')\n",
    "    print()\n",
    "    print(\"平均処理時間: \" + str(int((total_spent_time / (total-1))*1000.0)) + \" ms/枚\")\n",
    "    print(\"平均推論時間: \" + str(int((total_infer_spent_time / (total-1))*1000.0)) + \" ms/枚\")\n",
    "    return int((total_spent_time / (total-1))*1000.0), int((total_infer_spent_time / (total-1))*1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference('161.202.224.155', '9000', 'dogcat', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
